{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn #neural network\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as TF\n",
    "import torch.optim as optim # optimizer\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Cuda Device and Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set test device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>data_set</th>\n",
       "      <th>condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 img  data_set  condition\n",
       "0  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0\n",
       "1  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0\n",
       "2  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0\n",
       "3  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0\n",
       "4  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_df = pd.read_csv(\"../../data/image_index.csv\", index_col=0)\n",
    "index_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = index_df[index_df.data_set==0]\n",
    "train_df = index_df[index_df.data_set==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df[[\"img\", \"condition\"]].sample(30, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = torch.tensor(train_data.condition.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = train_data.img.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Linear Prototype with Flattened Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_prototype2(nn.Module):\n",
    "    def __init__(self, img_h, img_w):\n",
    "        super().__init__()\n",
    "        #define sizes here\n",
    "        self.h = img_h\n",
    "        self.w = img_w\n",
    "        self.longshape = img_h*img_w\n",
    "        self.linear1 = nn.Linear(self.longshape, 320)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(320, 2)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # preprocess the input image\n",
    "        x = self.preprocess_image_flat(x)\n",
    "        #============ Layer1==============#\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        #============Layer2==============#\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    ## update this to sequential?\n",
    "    def preprocess_image_flat(self, path):\n",
    "        resizer = TF.Resize((self.h, self.w)) #define resizer per new_h and new_w\n",
    "        im = tv.io.read_image(path).type(torch.float) #read image as pytorch float tensor\n",
    "        im = resizer(im) #resize image\n",
    "        normalizer = TF.Normalize(im.mean(), im.std()) #initialize normalizer\n",
    "        im = normalizer(im) # return normalized pytorch float tensor\n",
    "        return torch.flatten(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_prototype2(640, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_loss(y_true, y_pred):\n",
    "    answers = list(zip(y_true, y_pred))\n",
    "    true_positives = sum(1 if (true == 1 and pred == 1) else 0 for true, pred in answers)\n",
    "    false_negatives = sum(1 if (true == 1 and pred == 0) else 0 for true, pred in answers)\n",
    "    return torch.tensor(1 - (true_positives/(true_positives + false_negatives)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optim(n_seq, learning_rate, img_path_list):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for t in range(n_seq):\n",
    "        print(t)\n",
    "        y_pred = [model(path) for path in img_path_list]\n",
    "        y_pred = torch.tensor([1 if pred[0] > pred[1] else 0 for pred in y_pred]).float()\n",
    "        print(\"prediction\", y_pred)\n",
    "        print(\"true\", y_true)\n",
    "        loss = criterion(y_pred, y_true) #calculate loss\n",
    "\n",
    "        plt.scatter(t, loss)\n",
    "    \n",
    "        \n",
    "        optimizer.zero_grad() #reset gradient\n",
    "        print(\"it runs upto this line\")\n",
    "        # gradient back step\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        # update parameters per learning rate (go down the gradient)\n",
    "        with torch.no_grad(): #sequential, no backprop\n",
    "            for param in model.parameters():\n",
    "                param += learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "prediction tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.])\n",
      "true tensor([1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.])\n",
      "it runs upto this line\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-335-a2ba827ef8c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun_optim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_paths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-334-43f0a95b1116>\u001b[0m in \u001b[0;36mrun_optim\u001b[1;34m(n_seq, learning_rate, img_path_list)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"it runs upto this line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# gradient back step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fitsnbits\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fitsnbits\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_optim(5, 0.01, img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = torch.tensor([1,0,1,1,1,1,1,0,1,1,1,0]).float()\n",
    "y2 = torch.tensor([1,0,1,1,0,0,1,0,0,1,1,0]).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.sqrt(torch.mean((y1-y2)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3333)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_loss(y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [1, 1, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [0, 1, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = zip(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true 1\n",
      "pred 0\n",
      "true 1\n",
      "pred 1\n",
      "true 0\n",
      "pred 1\n",
      "true 1\n",
      "pred 1\n",
      "true 0\n",
      "pred 0\n"
     ]
    }
   ],
   "source": [
    "true_positive = 0\n",
    "false_negative = 0\n",
    "\n",
    "for true, pred in answer:\n",
    "    print(\"true\", true)\n",
    "    print(\"pred\", pred)\n",
    "    if pred == 1 and true == 1:\n",
    "        true_positive += 1\n",
    "    elif true == 1 and pred == 0:\n",
    "        false_negative += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_positive/(false_negative + true_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recallhit(cl, pred):\n",
    "    for x, y in list(zip(cl,pred)):\n",
    "        if (x == 1) and (y == 1):\n",
    "            return 1\n",
    "        elif (x == 1) and y == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Linear Prototype\n",
    "\n",
    "The issue with this model is that the output is `[1, 640, 2]` when the output should be a shape of `[2,]`. \n",
    "\n",
    "A flattening maybe required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_prototype(nn.Module):\n",
    "    def __init__(self, img_h, img_w):\n",
    "        super().__init__()\n",
    "        #define sizes here\n",
    "        self.h = img_h\n",
    "        self.w = img_w\n",
    "        self.linear1 = nn.Linear(img_w, 320)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(320, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # preprocess the input image\n",
    "        x = self.preprocess_image(x)\n",
    "        #============ Layer1==============#\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        #============Layer2==============#\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return self.softmax(x)\n",
    "    \n",
    "    ## update this to sequential?\n",
    "    def preprocess_image(self, path):\n",
    "        resizer = TF.Resize((self.h, self.w)) #define resizer per new_h and new_w\n",
    "        im = tv.io.read_image(path).type(torch.float) #read image as pytorch float tensor\n",
    "        im = resizer(im) #resize image\n",
    "        normalizer = TF.Normalize(im.mean(), im.std()) #initialize normalizer\n",
    "        return normalizer(im) # return normalized pytorch float tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_model = linear_prototype(640, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linear_prototype(\n",
       "  (linear1): Linear(in_features=640, out_features=320, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (linear2): Linear(in_features=320, out_features=2, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proto_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = proto_model(test_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 640, 2])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fitsnbits",
   "language": "python",
   "name": "fitsnbits"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
