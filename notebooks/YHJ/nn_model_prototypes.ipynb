{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook a simple sequential linear model will be buitl using pytorch neural network module. The model and input data will be pushed to a CUDA device for quick calculation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn #neural network\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as TF\n",
    "import torch.optim as optim # optimizer\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Cuda Device and Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set test device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>data_set</th>\n",
       "      <th>condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 img  data_set  condition\n",
       "0  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0\n",
       "1  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0\n",
       "2  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0\n",
       "3  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0\n",
       "4  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_df = pd.read_csv(\"../../data/image_index.csv\", index_col=0)\n",
    "index_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>data_set</th>\n",
       "      <th>condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 img  data_set  condition\n",
       "0  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0\n",
       "1  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0\n",
       "2  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0\n",
       "3  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0\n",
       "4  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = index_df[index_df.data_set==0]\n",
    "train_df = index_df[index_df.data_set==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df[[\"img\",\"condition\"]].sample(30, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = torch.tensor(train_data[\"condition\"].values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = train_data.img.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## update this to sequential?\n",
    "def preprocess_image_flat(path_list, img_h=120, img_w=120):\n",
    "    outlist = []\n",
    "    for path in path_list:\n",
    "        resizer = TF.Resize((img_h, img_w)) #define resizer per new_h and new_w\n",
    "        im = tv.io.read_image(path).type(torch.float) #read image as pytorch float tensor\n",
    "        im = resizer(im) #resize image\n",
    "        normalizer = TF.Normalize(im.mean(), im.std()) #initialize normalizer\n",
    "        im = normalizer(im) # return normalized pytorch float tensor\n",
    "        im = torch.flatten(im)\n",
    "        outlist.append(im)\n",
    "    return torch.stack(outlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = preprocess_image_flat(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = input_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4325, -1.4333, -1.4134,  ..., -1.4453, -1.4453, -1.4453],\n",
       "        [-1.9128, -1.8097, -1.6496,  ..., -2.0894, -2.0894, -2.0894],\n",
       "        [-1.3663, -1.2909, -1.2273,  ..., -1.4563,  0.6902,  0.7155],\n",
       "        ...,\n",
       "        [-0.3183, -0.2032, -1.0656,  ..., -1.6457, -1.6457, -1.6457],\n",
       "        [-0.8491, -0.8987, -1.0497,  ..., -1.5477, -1.2935, -0.6150],\n",
       "        [-1.7011, -1.3153, -1.1172,  ..., -2.0501, -2.0501, -2.0501]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_prototype3(nn.Module):\n",
    "    def __init__(self, img_h, img_w):\n",
    "        super().__init__()\n",
    "        #define sizes here\n",
    "        self.h = img_h\n",
    "        self.w = img_w\n",
    "        self.longshape = img_h*img_w\n",
    "        self.linear1 = nn.Linear(self.longshape, 320)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(320, 1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.termn_act = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # preprocess the input image\n",
    "        #============ Layer1==============#\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        #============Layer2==============#\n",
    "        x = self.linear2(x)\n",
    "        #x = self.softmax(x)\n",
    "        x = self.termn_act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_prototype4(nn.Module):\n",
    "    def __init__(self, img_h, img_w):\n",
    "        super().__init__()\n",
    "        #define sizes here\n",
    "        self.h = img_h\n",
    "        self.w = img_w\n",
    "        self.longshape = img_h*img_w\n",
    "        self.linear1 = nn.Linear(self.longshape, 320)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(320, 1)\n",
    "        self.term_act = nn.Sigmoid()\n",
    "        self.relu2 = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # preprocess the input image\n",
    "        #============ Layer1==============#\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        #============Layer2==============#\n",
    "        x = self.linear2(x)\n",
    "        #x = self.softmax(x)\n",
    "        x = self.term_act(x)\n",
    "        return self.relu2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linear_prototype4(\n",
       "  (linear1): Linear(in_features=14400, out_features=320, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (linear2): Linear(in_features=320, out_features=1, bias=True)\n",
       "  (term_act): Sigmoid()\n",
       "  (relu2): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = linear_prototype4(120, 120)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'linear_prototype3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-25dcfa937eb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_prototype3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m120\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m120\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'linear_prototype3' is not defined"
     ]
    }
   ],
   "source": [
    "model = linear_prototype3(120, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linear_prototype4(\n",
       "  (linear1): Linear(in_features=14400, out_features=320, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (linear2): Linear(in_features=320, out_features=1, bias=True)\n",
       "  (term_act): Sigmoid()\n",
       "  (relu2): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optim(n_seq, learning_rate, img_paths, y_true):\n",
    "    y_true = y_true.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for t in range(n_seq):\n",
    "\n",
    "        y_pred = model(img_paths).flatten()\n",
    "        loss = criterion(y_pred, y_true) #calculate loss\n",
    "        if t%50 == 0:\n",
    "            print(\"training epoch\", t)\n",
    "            loss_copy = loss.detach().cpu()\n",
    "        plt.scatter(t, loss_copy)\n",
    "        optimizer.zero_grad() #reset gradient)\n",
    "        \n",
    "        # gradient back step\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        \n",
    "        # update parameters per learning rate (go down the gradient)\n",
    "        with torch.no_grad(): #sequential\n",
    "            for param in model.parameters():\n",
    "                param += learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 0\n",
      "training epoch 50\n",
      "training epoch 100\n",
      "training epoch 150\n",
      "training epoch 200\n",
      "training epoch 250\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_optim(300, 0.001, input_data, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this model reduces loss to 0 in about ~100 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"../../models/pneumonia_simple_01.pt\"\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00],\n",
       "        [1.3422e-11],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [5.6273e-14],\n",
       "        [4.8645e-11],\n",
       "        [5.8783e-11],\n",
       "        [2.9276e-11],\n",
       "        [1.0000e+00],\n",
       "        [4.0300e-09],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.6256e-11],\n",
       "        [1.0000e+00],\n",
       "        [9.0260e-11],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.2012e-10],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [7.9408e-05],\n",
       "        [1.6465e-09],\n",
       "        [3.8385e-17]], device='cuda:0', grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Old Model Iteration\n",
    "\n",
    "These are saved for future reference and/or troubleshooting\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Linear Prototype\n",
    "\n",
    "The issue with this model is that the output is `[1, 640, 2]` when the output should be a shape of `[2,]`. \n",
    "\n",
    "A flattening maybe required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_prototype(nn.Module):\n",
    "    def __init__(self, img_h, img_w):\n",
    "        super().__init__()\n",
    "        #define sizes here\n",
    "        self.h = img_h\n",
    "        self.w = img_w\n",
    "        self.linear1 = nn.Linear(img_w, 320)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(320, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # preprocess the input image\n",
    "        x = self.preprocess_image(x)\n",
    "        #============ Layer1==============#\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        #============Layer2==============#\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return self.softmax(x)\n",
    "    \n",
    "    ## update this to sequential?\n",
    "    def preprocess_image(self, path):\n",
    "        resizer = TF.Resize((self.h, self.w)) #define resizer per new_h and new_w\n",
    "        im = tv.io.read_image(path).type(torch.float) #read image as pytorch float tensor\n",
    "        im = resizer(im) #resize image\n",
    "        normalizer = TF.Normalize(im.mean(), im.std()) #initialize normalizer\n",
    "        return normalizer(im) # return normalized pytorch float tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_model = linear_prototype(640, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linear_prototype(\n",
       "  (linear1): Linear(in_features=640, out_features=320, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (linear2): Linear(in_features=320, out_features=2, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proto_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = proto_model(test_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 640, 2])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Model Optimizer\n",
    "\n",
    "Runs into tensor placement errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optim2(n_seq, learning_rate, img_path_list):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for t in range(n_seq):\n",
    "        y_pred = [model(path) for path in img_path_list]\n",
    "        y_pred = torch.tensor([1 if pred[0] > pred[1] else 0 for pred in y_pred]).float()\n",
    "        y_pred = y_pred.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        loss = criterion(y_pred, y_true) #calculate loss\n",
    "\n",
    "        plt.scatter(t, loss.detach())\n",
    "        optimizer.zero_grad() #reset gradient)\n",
    "        \n",
    "        # gradient back step\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        \n",
    "        # update parameters per learning rate (go down the gradient)\n",
    "        with torch.no_grad(): #sequential\n",
    "            for param in model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    param += learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Linear Prototype with Flattened Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_prototype2(nn.Module):\n",
    "    def __init__(self, img_h, img_w):\n",
    "        super().__init__()\n",
    "        #define sizes here\n",
    "        self.h = img_h\n",
    "        self.w = img_w\n",
    "        self.longshape = img_h*img_w\n",
    "        self.linear1 = nn.Linear(self.longshape, 320)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(320, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # preprocess the input image\n",
    "        x = self.preprocess_image_flat(x)\n",
    "        #============ Layer1==============#\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        #============Layer2==============#\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "    ## update this to sequential?\n",
    "    def preprocess_image_flat(self, path):\n",
    "        resizer = TF.Resize((self.h, self.w)) #define resizer per new_h and new_w\n",
    "        im = tv.io.read_image(path).type(torch.float) #read image as pytorch float tensor\n",
    "        im = resizer(im) #resize image\n",
    "        normalizer = TF.Normalize(im.mean(), im.std()) #initialize normalizer\n",
    "        im = normalizer(im) # return normalized pytorch float tensor\n",
    "        im = torch.flatten(im)\n",
    "        return im.clone().detach().requires_grad_(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fitsnbits",
   "language": "python",
   "name": "fitsnbits"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
