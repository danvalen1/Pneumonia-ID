{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook a simple sequential linear model will be buitl using pytorch neural network module. The model and input data will be pushed to a CUDA device for quick calculation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn #neural network\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as TF\n",
    "import torch.optim as optim # optimizer\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Cuda Device and Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set test device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>data_set</th>\n",
       "      <th>condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 img  data_set  condition\n",
       "0  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0\n",
       "1  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0\n",
       "2  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0\n",
       "3  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0\n",
       "4  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_df = pd.read_csv(\"../../data/image_index.csv\", index_col=0)\n",
    "index_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>data_set</th>\n",
       "      <th>condition</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 img  data_set  condition  \\\n",
       "0  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0   \n",
       "1  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0   \n",
       "2  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0   \n",
       "3  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0   \n",
       "4  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0   \n",
       "\n",
       "   negative  positive  \n",
       "0         1         0  \n",
       "1         1         0  \n",
       "2         1         0  \n",
       "3         1         0  \n",
       "4         1         0  "
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = index_df[index_df.data_set==0]\n",
    "train_df = index_df[index_df.data_set==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df[[\"img\",\"condition\"]].sample(30, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = torch.tensor(train_data[\"condition\"].values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = train_data.img.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30])"
      ]
     },
     "execution_count": 710,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "## update this to sequential?\n",
    "def preprocess_image_flat(path_list, img_h=120, img_w=120):\n",
    "    outlist = []\n",
    "    for path in path_list:\n",
    "        resizer = TF.Resize((img_h, img_w)) #define resizer per new_h and new_w\n",
    "        im = tv.io.read_image(path).type(torch.float) #read image as pytorch float tensor\n",
    "        im = resizer(im) #resize image\n",
    "        normalizer = TF.Normalize(im.mean(), im.std()) #initialize normalizer\n",
    "        im = normalizer(im) # return normalized pytorch float tensor\n",
    "        im = torch.flatten(im)\n",
    "        outlist.append(im)\n",
    "    return torch.stack(outlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = preprocess_image_flat(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = input_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4325, -1.4333, -1.4134,  ..., -1.4453, -1.4453, -1.4453],\n",
       "        [-1.9128, -1.8097, -1.6496,  ..., -2.0894, -2.0894, -2.0894],\n",
       "        [-1.3663, -1.2909, -1.2273,  ..., -1.4563,  0.6902,  0.7155],\n",
       "        ...,\n",
       "        [-0.3183, -0.2032, -1.0656,  ..., -1.6457, -1.6457, -1.6457],\n",
       "        [-0.8491, -0.8987, -1.0497,  ..., -1.5477, -1.2935, -0.6150],\n",
       "        [-1.7011, -1.3153, -1.1172,  ..., -2.0501, -2.0501, -2.0501]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_prototype3(nn.Module):\n",
    "    def __init__(self, img_h, img_w):\n",
    "        super().__init__()\n",
    "        #define sizes here\n",
    "        self.h = img_h\n",
    "        self.w = img_w\n",
    "        self.longshape = img_h*img_w\n",
    "        self.linear1 = nn.Linear(self.longshape, 320)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(320, 1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.termn_act = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # preprocess the input image\n",
    "        #============ Layer1==============#\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        #============Layer2==============#\n",
    "        x = self.linear2(x)\n",
    "        #x = self.softmax(x)\n",
    "        x = self.termn_act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_prototype3(120, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linear_prototype3(\n",
       "  (linear1): Linear(in_features=14400, out_features=320, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (linear2): Linear(in_features=320, out_features=1, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       "  (termn_act): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 736,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 1])"
      ]
     },
     "execution_count": 738,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30])"
      ]
     },
     "execution_count": 739,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optim(n_seq, learning_rate, img_paths, y_true):\n",
    "    y_true = y_true.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for t in range(n_seq):\n",
    "\n",
    "        y_pred = model(img_paths).flatten()\n",
    "        loss = criterion(y_pred, y_true) #calculate loss\n",
    "        if t%100 == 0:\n",
    "            print(\"training epoch\", t)\n",
    "            loss_copy = loss.detach().cpu()\n",
    "        plt.scatter(t, loss_copy)\n",
    "        optimizer.zero_grad() #reset gradient)\n",
    "        \n",
    "        # gradient back step\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        \n",
    "        # update parameters per learning rate (go down the gradient)\n",
    "        with torch.no_grad(): #sequential\n",
    "            for param in model.parameters():\n",
    "                param += learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 0\n",
      "training epoch 100\n",
      "training epoch 200\n",
      "training epoch 300\n",
      "training epoch 400\n",
      "training epoch 500\n",
      "training epoch 600\n",
      "training epoch 700\n",
      "training epoch 800\n",
      "training epoch 900\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_optim(1000, 0.001, input_data, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this model reduces loss to 0 in about ~100 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"../../models/pneumonia_simple_01.pt\"\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Old Model Iteration\n",
    "\n",
    "These are saved for future reference and/or troubleshooting\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Linear Prototype\n",
    "\n",
    "The issue with this model is that the output is `[1, 640, 2]` when the output should be a shape of `[2,]`. \n",
    "\n",
    "A flattening maybe required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_prototype(nn.Module):\n",
    "    def __init__(self, img_h, img_w):\n",
    "        super().__init__()\n",
    "        #define sizes here\n",
    "        self.h = img_h\n",
    "        self.w = img_w\n",
    "        self.linear1 = nn.Linear(img_w, 320)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(320, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # preprocess the input image\n",
    "        x = self.preprocess_image(x)\n",
    "        #============ Layer1==============#\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        #============Layer2==============#\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return self.softmax(x)\n",
    "    \n",
    "    ## update this to sequential?\n",
    "    def preprocess_image(self, path):\n",
    "        resizer = TF.Resize((self.h, self.w)) #define resizer per new_h and new_w\n",
    "        im = tv.io.read_image(path).type(torch.float) #read image as pytorch float tensor\n",
    "        im = resizer(im) #resize image\n",
    "        normalizer = TF.Normalize(im.mean(), im.std()) #initialize normalizer\n",
    "        return normalizer(im) # return normalized pytorch float tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_model = linear_prototype(640, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linear_prototype(\n",
       "  (linear1): Linear(in_features=640, out_features=320, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (linear2): Linear(in_features=320, out_features=2, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proto_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = proto_model(test_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 640, 2])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Model Optimizer\n",
    "\n",
    "Runs into tensor placement errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optim2(n_seq, learning_rate, img_path_list):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for t in range(n_seq):\n",
    "        y_pred = [model(path) for path in img_path_list]\n",
    "        y_pred = torch.tensor([1 if pred[0] > pred[1] else 0 for pred in y_pred]).float()\n",
    "        y_pred = y_pred.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        loss = criterion(y_pred, y_true) #calculate loss\n",
    "\n",
    "        plt.scatter(t, loss.detach())\n",
    "        optimizer.zero_grad() #reset gradient)\n",
    "        \n",
    "        # gradient back step\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        \n",
    "        # update parameters per learning rate (go down the gradient)\n",
    "        with torch.no_grad(): #sequential\n",
    "            for param in model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    param += learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Linear Prototype with Flattened Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_prototype2(nn.Module):\n",
    "    def __init__(self, img_h, img_w):\n",
    "        super().__init__()\n",
    "        #define sizes here\n",
    "        self.h = img_h\n",
    "        self.w = img_w\n",
    "        self.longshape = img_h*img_w\n",
    "        self.linear1 = nn.Linear(self.longshape, 320)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(320, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # preprocess the input image\n",
    "        x = self.preprocess_image_flat(x)\n",
    "        #============ Layer1==============#\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        #============Layer2==============#\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "    ## update this to sequential?\n",
    "    def preprocess_image_flat(self, path):\n",
    "        resizer = TF.Resize((self.h, self.w)) #define resizer per new_h and new_w\n",
    "        im = tv.io.read_image(path).type(torch.float) #read image as pytorch float tensor\n",
    "        im = resizer(im) #resize image\n",
    "        normalizer = TF.Normalize(im.mean(), im.std()) #initialize normalizer\n",
    "        im = normalizer(im) # return normalized pytorch float tensor\n",
    "        im = torch.flatten(im)\n",
    "        return im.clone().detach().requires_grad_(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fitsnbits",
   "language": "python",
   "name": "fitsnbits"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
