{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn #neural network\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as TF\n",
    "import torch.optim as optim # optimizer\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Cuda Device and Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set test device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>data_set</th>\n",
       "      <th>condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 img  data_set  condition\n",
       "0  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0\n",
       "1  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0\n",
       "2  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0\n",
       "3  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0\n",
       "4  ..\\..\\data\\extracted\\chest_xray\\test\\NORMAL\\IM...         0          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_df = pd.read_csv(\"../../data/image_index.csv\", index_col=0)\n",
    "index_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_df['img'] = [img.replace('\\\\', '/') for img in index_df['img']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = index_df[index_df.data_set==0]\n",
    "train_df = index_df[index_df.data_set==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df[[\"img\", \"condition\"]].sample(30, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = torch.tensor(train_data.condition.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielvalenzuela/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "y_true = torch.tensor(y_true, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = train_data.img.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Linear Prototype with Flattened Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_prototype2(nn.Module):\n",
    "    def __init__(self, img_h, img_w):\n",
    "        super().__init__()\n",
    "        #define sizes here\n",
    "        self.h = img_h\n",
    "        self.w = img_w\n",
    "        self.longshape = img_h*img_w\n",
    "        self.linear1 = nn.Linear(self.longshape, 320)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(320, 2)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # preprocess the input image\n",
    "        x = self.preprocess_image_flat(x)\n",
    "        #============ Layer1==============#\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        #============Layer2==============#\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    ## update this to sequential?\n",
    "    def preprocess_image_flat(self, path):\n",
    "        resizer = TF.Resize((self.h, self.w)) #define resizer per new_h and new_w\n",
    "        im = tv.io.read_image(path).type(torch.float) #read image as pytorch float tensor\n",
    "        im = resizer(im) #resize image\n",
    "        normalizer = TF.Normalize(im.mean(), im.std()) #initialize normalizer\n",
    "        im = normalizer(im) # return normalized pytorch float tensor\n",
    "        return torch.flatten(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_prototype2(640, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_loss(y_true, y_pred):\n",
    "    answers = list(zip(y_true, y_pred))\n",
    "    true_positives = sum(1 if (true == 1 and pred == 1) else 0 for true, pred in answers)\n",
    "    false_negatives = sum(1 if (true == 1 and pred == 0) else 0 for true, pred in answers)\n",
    "    return torch.tensor(1 - (true_positives/(true_positives + false_negatives)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optim(n_seq, learning_rate, img_path_list):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for t in range(n_seq):\n",
    "        print(t)\n",
    "        y_pred = [model(path) for path in img_path_list]\n",
    "        y_pred = torch.tensor([1 if pred[0] > pred[1] else 0 for pred in y_pred]).float()\n",
    "        y_pred = y_pred.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        print(\"prediction\", y_pred)\n",
    "        print(\"true\", y_true)\n",
    "        loss = criterion(y_pred, y_true) #calculate loss\n",
    "\n",
    "        plt.scatter(t, loss.detach().numpy())\n",
    "    \n",
    "        \n",
    "        optimizer.zero_grad() #reset gradient\n",
    "        print(\"it runs upto this line\")\n",
    "        # gradient back step\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        # update parameters per learning rate (go down the gradient)\n",
    "        with torch.no_grad(): #sequential, no backprop\n",
    "            for param in model.parameters():\n",
    "                param += learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "prediction tensor([1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1.], requires_grad=True)\n",
      "true tensor([1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.])\n",
      "it runs upto this line\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-a2ba827ef8c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_optim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-e60c97765a3d>\u001b[0m in \u001b[0;36mrun_optim\u001b[0;34m(n_seq, learning_rate, img_path_list)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#sequential, no backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0mparam\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_optim(5, 0.01, img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = torch.tensor([1,0,1,1,1,1,1,0,1,1,1,0]).float()\n",
    "y2 = torch.tensor([1,0,1,1,0,0,1,0,0,1,1,0]).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.sqrt(torch.mean((y1-y2)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3333)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_loss(y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [1, 1, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [0, 1, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = zip(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true 1\n",
      "pred 0\n",
      "true 1\n",
      "pred 1\n",
      "true 0\n",
      "pred 1\n",
      "true 1\n",
      "pred 1\n",
      "true 0\n",
      "pred 0\n"
     ]
    }
   ],
   "source": [
    "true_positive = 0\n",
    "false_negative = 0\n",
    "\n",
    "for true, pred in answer:\n",
    "    print(\"true\", true)\n",
    "    print(\"pred\", pred)\n",
    "    if pred == 1 and true == 1:\n",
    "        true_positive += 1\n",
    "    elif true == 1 and pred == 0:\n",
    "        false_negative += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_positive/(false_negative + true_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recallhit(cl, pred):\n",
    "    for x, y in list(zip(cl,pred)):\n",
    "        if (x == 1) and (y == 1):\n",
    "            return 1\n",
    "        elif (x == 1) and y == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Linear Prototype\n",
    "\n",
    "The issue with this model is that the output is `[1, 640, 2]` when the output should be a shape of `[2,]`. \n",
    "\n",
    "A flattening maybe required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_prototype(nn.Module):\n",
    "    def __init__(self, img_h, img_w):\n",
    "        super().__init__()\n",
    "        #define sizes here\n",
    "        self.h = img_h\n",
    "        self.w = img_w\n",
    "        self.linear1 = nn.Linear(img_w, 320)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(320, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # preprocess the input image\n",
    "        x = self.preprocess_image(x)\n",
    "        #============ Layer1==============#\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        #============Layer2==============#\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return self.softmax(x)\n",
    "    \n",
    "    ## update this to sequential?\n",
    "    def preprocess_image(self, path):\n",
    "        resizer = TF.Resize((self.h, self.w)) #define resizer per new_h and new_w\n",
    "        im = tv.io.read_image(path).type(torch.float) #read image as pytorch float tensor\n",
    "        im = resizer(im) #resize image\n",
    "        normalizer = TF.Normalize(im.mean(), im.std()) #initialize normalizer\n",
    "        return normalizer(im) # return normalized pytorch float tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_model = linear_prototype(640, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linear_prototype(\n",
       "  (linear1): Linear(in_features=640, out_features=320, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (linear2): Linear(in_features=320, out_features=2, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proto_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_image_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-f19c2f9f5101>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproto_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_image_path' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = proto_model(test_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 640, 2])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
